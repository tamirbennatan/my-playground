---
title: "Managing and Exploring Many Models using `tidyr`, `purrr` and `broom`"
output:
  html_document:
    fig_height: 6
    fig_width: 8
    messages: no
    toc: true
    toc_depth: 3
    df_print: paged
---

```{R}
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
library(purrr)
library(reshape2)
library(stringr)
library(lubridate)
library(forecast)

```

## 1. Introduction

In the 2016 _PlotCon_ data visualization conference, Hadley Whickham - chief scientist at RStudio and the mastermind behind [my favorite R packages](https://www.tidyverse.org/) - gave [a great talk](https://www.youtube.com/watch?v=cU0-NrUxRw4) about how to use a simple idea of _list columns_ and some tools from functional programming to manage and explore many models at once in a tidy fashion. 

What I like about this talk - and the proposed technique (which I use in this notebook) - is that Whickham shows how to muse _modeling as a form of data exploration_. From my experience, statistical modeling does not usually help you discover a new dimension or property in your data. By definition, statistical models are limited to the data and features you train them on, and since their behavior is so closely tied to your initial assumptions, they rarely make you question your assumptions or teach you something about your dataset. 

Whickham proposes a simple but powerful technique to use modeling as a tool to _understand and learn about your data_ - not only as a means to form predictions or formalize statistical properties. If one is interested in learning about the behavior of different groups of data  within a dataset (in this case, the groups are different Wikipedia pages), Whickham suggests to create a dataframe where there is one row per group, and all the data associated with that group is stored in a column of dataframes. The result is a dataframe of dataframes. Then, one can apply modeling techniques to each group in parrallel, and use corresponding measures of fitness or properties of the parameters to learn about each group. 

In this notebook, I use this technique to fit statistical models to the time series of each Wikipedia article in parralel. I then use the properties of these models to try and learn about the time series themselves. 

Note: I do not think that this approach is the only or best way to learn about the properties of many time series at once. I do think it is an interesting approach, and a technique that in any setting in which you want to model your data based on the group each data point belongs to. 

## 1. Wikipedia Data - a first look

First, I'll need to load the data, and reshape it so that it's easier to work with. 


#### 1.1 Loading data

```{R}
# Training data
data <- read.csv("data/train_2.csv")
```

#### 1.2 Making the data Tidy

At first glance of the data:

```{R}
head(data)
```


We can see that each row contains the name of the Wikipedia article under the `Page` column, and the remaining 803 columns are page views for that page on different dates.

The first step whenever I perform an analysis with the [tidyverse stack](https://www.tidyverse.org/) is convert the data into a **tidy** format, meaning that:

1. Every column represents one variable
2. Every row represents one observations. 

Here, the true varaibles in this data are:

- Page name
- Date
- Number of Views

And a single observation is the number of page views recorded for a single page on a single day.

Using the `reshape2` package, we can convert the data so that it has this exact form:


```{R}
# convert to long format. 
data <- data %>%
      melt(id.vars = "Page") %>%
      rename(date = variable, 
             views = value)

head(data)
```

Now, our dataframe of ~145,000 rows and ~800 columns has been reshaped to a dataframe of ~116,000,000 rows and 3 columns.

#### 1.3 Consolidating data types

The next step is to make sure each of the columns are of the right data type. 

```{R}
str(data)
```

`Page` is a factor, and `views` is an integer datatype, which makes sense. `date` is stored as a factor, which is not right - really we want to store it as a native date format. 

To do this, we'll have to remove the leading `X` characters at the begining of each value. Later, I will cast these strings to  native `date` datatypes using the function `lubridate::ymd()` function - but for now I will leave it as strings - for reasons I will discuss in a minute. 

```{R}
data <- data %>%
      mutate(date = str_replace(pattern = "X", replacement = "", date))

head(data)
```


#### 1.4 Missing values

Just so we're aware - how many values are missing? 


```{R}
sum(is.na(data$views)) / nrow(data)
```

Around 6 percent of the view counts are missing. We can't be sure if this means that there were zero views for that page on that day, or if the view count data is simply missing for that day. 

Another relevent question is the proportion of of series which have _any_ missing values:

```{R}
# Look at the proportion of wikipedia pages that have one or more missing values. 
data %>% 
      group_by(Page) %>%
      summarize(num.missing = sum(is.na(views)) ) %>%
      mutate(any.missing = num.missing > 0) %>%
      .$any.missing %>% 
      mean()
```

So 20% of the Wikipedia page counts have at least one missing value. This is a little disconcerting, as some of the more classical forecasting methods (ARIMA, state space models) do not handle missing values gracefully. 

For forecasting purpose, I might have to resort to Facebook's `Prophet` package, which claims to be "robust to missing data". 



## 2. Nesting `dataframes` - a `dataframe` for every page. 

Now, as discussed earlier, we transform the dataset into a dataframe such that every page populats one row, and the corresponding data is stored in a list column. This is a three-liner with the `tidyr::nest()` function. 

#### 2.1 Using the `tidyr::nest()` function.

**A word of warning** - creating list columns with the `tidyr::nest()` function is very slow if if one of your columns is of type `Date` (see [this Github issue for more details](https://github.com/tidyverse/tidyr/issues/369)). As such, I will on

```{R}
# dataframes within dataframes... dataframeception!
nested = data %>%
      group_by(Page) %>%
      nest()
```


Now, taking a look at our nested dataframe, we can see that there is one row per Wikipedia page, and a column called `data` which is populated with a tibble (well behaved dataframe) containing the data fro that Wikipedia page:

```{R}
# first five wikipedia pages
head(nested)
```

```{R}
# isolating the first page in the dataset
first_page = nested[1,]

# taking a look at the first entry fo the column `data`
first_page$data
```

#### 2.2 Casting date types

Now that we have a properly neste dataframe, we can cast the `date` column in every nested dataframe to the correct datatype. 

Again, the reason I didn't do this upfront is because the `nest()` function is _very_ slow when one of the columns you are nesting is a date type. 

To do this type cast, I'll write small function that casts the data type of the `date` column in a dataframe, and then map it over all the nested dataframe using `purrr::map()`:

```{R}
# quick function to cast string dates to native dates
cast_date <- function(df){
      df %>% mutate(date = ymd(date))     
}
```

```{R}
# applying this transformation to each of the nested dataframes by
# modifying the `data` column, using `dplyr::mutate()`:
nested = nested %>%
      mutate(data = map(.f = cast_date, .x = data))

# isolating the first page in the dataset
first_page = nested[1,]
```


## 3. Before we start modeling... what's the data look like? 

The point of this notebook is to show how to use modeling for data exploration. But even so - I can't jump into modeling without knowing what the data looks like at the least.

I'm most interested in identifying structure in the data that will influence my choices of parameters in models to come, or which models to use. For example, I'd be interested in identifying are any seasonal structure at a human interval - such as weekly or annual seasonality - which I've come to expect with any time series data relating to online activity. This is because if I choose to fit a Seasonal ARIMA model to each time series, or perform some sort of time series decomposition, I will need to know the periodicity of the seasonality, if any. I'd also be interested in identifying any outliers or extreme values. This may motivate me to transform my data in some way, or use models that are robust to extreme values. 

#### 3.1 A first glance

So far, our data consists of one row per Wikipedia page, with a column containing the data for each page:


```{R}
head(nested)
```

Each nested tibble (the data associated with a page) consists of the view count over the dates in the dataset:

```{R}
nested[1,] %>%
      .$data %>%
      .[[1]] %>%
      head()
```

Some interesting questions are:

1. What information is stored in those convoluted page names?
2. What seasonality can we observe in the page views?
3. How variable are the page views across the different pages?
4. How many pages have very extreme page view counts? 


#### 3.2 Disecting the page names

Taking a closer look at a sample of 30 page names, we can see that there is a lot of information stored in those names...

```{R}
set.seed(1)
nested %>%
      sample_n(size = 30) %>%
      select(Page)

```

Some of the things I've noticed:

1. It looks like the the begining of the name is the Wikipedia page's title - spaces seperated by underscores. I see values of `Page` that start with strings like *Mouammar_Kadhafi* and *Masacre_de_la_Escuela_Secundaria_de_Columbine* (an article about the Columbine shooting in Spanish).
2. After the article title, there is a two letter code for the language (_ja, de, fr, en_, etc), followed by a period. 
3. The end of the page name shows the client the views were made on (e.g. _mobile, web, all-access_)
4. The very last words of the page name (from this sample) are ither *all-agents* or *spider*. Not sure what this is. 

The first thing I'd be interested in is extracting the language of the article. Studying the distribution of vies over different languages is as close as we can get to studying the viewing behavior of different geographic regions, which is an interesting venture. 

Using a simple regular expression, I can extract the two letters that precede the first period in the page - in hopes that the pattern I've seen in this sample extend to the rest of the data. 

```{R}
nested = nested %>%
      # a positive lookahead - extract two letters preceding first period. 
      mutate(language = str_match(pattern = regex("[a-z][a-z](?=\\.)"), Page))

```

```{R}
nested %>%
      count(language, sort = TRUE) %>%
      rename(count = n) %>%
      mutate(cumulative.proportion = cumsum(count)/sum(count)) %>%
      mutate(language = ifelse(cumulative.proportion < .9, language, "other")) %>%
      group_by(language) %>%
      summarize(count = sum(count)) %>%
      ungroup() %>%
      mutate(proportion = count/sum(count)) %>%
      ggplot(aes(x = language, y = proportion, fill = proportion)) +
      geom_col() +
      ylab("Proportion of pages with hypothesized language")

```

Using this extraction method, we can see that around 90% of the pages are in the languages:

1. German (de)
2. English (en)
3. Spanish (es)
4. French (fr)
5. Japanese (ja)
6. Russian (ru)
7. Chinese (zh)

The remaining 10 percent are "other". These could indeed be articles in different languges than the 7 listed above, or it could be that my extraction regex doesn't work correctly on the entire dataset. 

As such, I'll encode the languages not in the list above as *other*, and tread carefully when analyzing data of this class in the future:

```{R}
nested = nested %>%
      mutate(language = ifelse(
            language %in% c("de", "en", "es", "fr", "ja", "ru", "zh"),
            language, 
            "other"))
```



```{R}
apply_lm <- function(df){
      lm(data = df, views ~ date)     
}
```

```{R}
first_page %>%
      mutate(linear_reg = map(.f = apply_lm, .x = data)) %>%
      .$linear_reg %>%
      .[[1]] %>%
      summary()
```













































