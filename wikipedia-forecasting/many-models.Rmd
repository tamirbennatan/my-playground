---
title: "Managing and Exploring Many Models using `tidyr`, `purrr` and `broom`"
output:
  html_document:
    fig_height: 6
    fig_width: 8
    messages: no
    toc: true
    toc_depth: 3
    df_print: paged
---

```{R}
library(dplyr)
library(tidyr)
library(broom)
library(ggplot2)
library(purrr)
library(reshape2)
library(stringr)
library(lubridate)
library(forecast)

```

## 1. Introduction

In the 2016 _PlotCon_ data visualization conference, Hadley Whickham - chief scientist at RStudio and the mastermind behind [my favorite R packages](https://www.tidyverse.org/) - gave [a great talk](https://www.youtube.com/watch?v=cU0-NrUxRw4) about how to use a simple idea of _list columns_ and some tools from functional programming to manage and explore many models at once in a tidy fashion. 

What I like about this talk - and the proposed technique (which I use in this notebook) - is that Whickham shows how to muse _modeling as a form of data exploration_. From my experience, statistical modeling does not usually help you discover a new dimension or property in your data. By definition, statistical models are limited to the data and features you train them on, and since their behavior is so closely tied to your initial assumptions, they rarely make you question your assumptions or teach you something about your dataset. 

Whickham proposes a simple but powerful technique to use modeling as a tool to _understand and learn about your data_ - not only as a means to form predictions or formalize statistical properties. If one is interested in learning about the behavior of different groups of data  within a dataset (in this case, the groups are different Wikipedia pages), Whickham suggests to create a dataframe where there is one row per group, and all the data associated with that group is stored in a column of dataframes. The result is a dataframe of dataframes. Then, one can apply modeling techniques to each group in parrallel, and use corresponding measures of fitness or properties of the parameters to learn about each group. 

In this notebook, I use this technique to fit statistical models to the time series of each Wikipedia article in parralel. I then use the properties of these models to try and learn about the time series themselves. 

Note: I do not think that this approach is the only or best way to learn about the properties of many time series at once. I do think it is an interesting approach, and a technique that in any setting in which you want to model your data based on the group each data point belongs to. 

## 1. Wikipedia Data - a first look

First, I'll need to load the data, and reshape it so that it's easier to work with. 


#### 1.1 Loading data

```{R}
# Training data
data <- read.csv("data/train_2.csv")
```

#### 1.2 Making the data Tidy

At first glance of the data:

```{R}
head(data)
```


We can see that each row contains the name of the Wikipedia article under the `Page` column, and the remaining 803 columns are page views for that page on different dates.

The first step whenever I perform an analysis with the [tidyverse stack](https://www.tidyverse.org/) is convert the data into a **tidy** format, meaning that:

1. Every column represents one variable
2. Every row represents one observations. 

Here, the true varaibles in this data are:

- Page name
- Date
- Number of Views

And a single observation is the number of page views recorded for a single page on a single day.

Using the `reshape2` package, we can convert the data so that it has this exact form:


```{R}
# convert to long format. 
data <- data %>%
      melt(id.vars = "Page") %>%
      rename(date = variable, 
             views = value)

head(data)
```

Now, our dataframe of ~145,000 rows and ~800 columns has been reshaped to a dataframe of ~116,000,000 rows and 3 columns.

#### 1.3 Consolidating data types

The next step is to make sure each of the columns are of the right data type. 

```{R}
str(data)
```

`Page` is a factor, and `views` is an integer datatype, which makes sense. `date` is stored as a factor, which is not right - really we want to store it as a native date format. 

To do this, we'll have to remove the leading `X` characters at the begining of each value, and cast it to a native `date` datatype using the function `lubridate::date`. 

```{R}
data <- data %>%
      mutate(date = str_replace(pattern = "X", replacement = "", date)) %>%
      mutate(date = ymd(date))

head(data)
```


#### 1.4 Missing values

Just so we're aware - how many values are missing? 


```{R}
sum(is.na(data$views)) / nrow(data)
```

Around 6 percent of the view counts are missing. We can't be sure if this means that there were zero views for that page on that day, or if the view count data is simply missing for that day. 

Another relevent question is the proportion of of series which have _any_ missing values:

```{R}
# Look at the proportion of wikipedia pages that have one or more missing values. 
data %>% 
      group_by(Page) %>%
      summarize(num.missing = sum(is.na(views)) ) %>%
      mutate(any.missing = num.missing > 0) %>%
      .$any.missing %>% 
      mean()
```

So 20% of the Wikipedia page counts have at least one missing value. This is a little disconcerting, as some of the more classical forecasting methods (ARIMA, state space models) do not handle missing values gracefully. 

For forecasting purpose, I might have to resort to Facebook's `Prophet` package, which claims to be "robust to missing data". 















































