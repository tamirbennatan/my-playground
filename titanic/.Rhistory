df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
suppressMessages(require(reshape2))
melt(data = (
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
))
melt(data = (
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
), id.vars = c("Sex", "Parch"))
melt(data = (
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
), id.vars = c("Sex", "Parch"), measure.vars = "survival.rate")
melt(data = (
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
), id.vars = c("Pclas", "Sex", "Parch"), measure.vars = "survival.rate")
melt(data = (
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
), id.vars = c("Pclass", "Sex", "Parch"), measure.vars = "survival.rate")
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
temp
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
melt(temp, id.vars = "Parch")
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
temp
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = SibSp) +  geom_bar(aes(fill = Sex), position = "dodge")) + facet_wrap(~Pclass)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = SibSp) +  geom_bar(aes(fill = Sex)), position = "dodge") + facet_wrap(~Pclass)
knitr::opts_chunk$set(echo = TRUE)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
library(dplyr)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = SibSp) +  geom_bar(aes(fill = Sex)) + facet_wrap(~Pclass)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = SibSp)) +  geom_bar(aes(fill = Sex)) + facet_wrap(~Pclass)
library(ggplo2)
library(ggplot2)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = SibSp)) +  geom_bar(aes(fill = Sex)) + facet_wrap(~Pclass)
temp
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = survival.rate)) +  geom_bar(aes(fill = Sex)) + facet_wrap(~Pclass)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = survival.rate)) +  geom_bar(aes(fill = Sex), stat = "identity") + facet_wrap(~Pclass)
source('~/.active-rstudio-document', echo=TRUE)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = survival.rate)) +  geom_bar(aes(fill = Sex), position = "dodge", stat = "identity") + facet_wrap(~Pclass)
temp <- df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
ggplot(data = temp, aes(x = Parch, y = survival.rate)) +  geom_bar(aes(fill = Sex), position = "dodge", stat = "identity") + facet_wrap(~Pclass)
adult.survival.by.children.spouses.class <-
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
g1 <- ggplot(data = adult.survival.by.children.spouses.class, aes(x = Parch, y = survival.rate)) +  geom_bar(aes(fill = Sex), position = "dodge", stat = "identity") + facet_wrap(~Pclass)
g2 <- ggplot(data = adult.survival.by.children.spouses.class, aes(x = Parch, y = num.passengers)) +  geom_bar(aes(fill = Sex), position = "dodge", stat = "identity") + facet_wrap(~Pclass)
grid.arrange(g1, g2, nrow = 2)
adult.survival.by.children.spouses.class <-
df.train %>%
filter(Age > 21) %>%
group_by(Parch, Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
g1 <- ggplot(data = adult.survival.by.children.spouses.class, aes(x = Parch, y = survival.rate)) +  geom_bar(aes(fill = Sex), position = "dodge", stat = "identity") + facet_wrap(~Pclass)
g2 <- ggplot(data = adult.survival.by.children.spouses.class, aes(x = Parch, y = num.passengers)) +  geom_bar(aes(fill = Sex), position = "dodge", stat = "identity") + facet_wrap(~Pclass)
grid.arrange(g1, g2, nrow = 2)
df.train %>%
group_by(Pclass, Sex) %>%
summarize(num.passengers = n(), survival.rate = mean(Survived))
df.train %>% filter(Sex == "male", Pclass = 3)
df.train %>% filter(Sex == "male", Pclass == 3)
ggplot(df.train %>% filter(Sex == "male", Pclass == 3)
aes(x = Fare, y = Survived)) + geom_line()
ggplot(data = (df.train %>% filter(Sex == "male", Pclass == 3)), aes(x = Fare, y = Survived)) + geom_line()
ggplot(data = (df.train %>% filter(Sex == "male", Pclass == 3)), aes(x = Fare, y = Survived)) + geom_point()
x ~y
clas(x~y)
class(x~y)
dim(df.train)
.8*891
?randperm
rperm
perm
c()
c(c(), 1)
c(c(c(), 1), 2)
sample(1:nrow(df.train), round(.8*nrow(df.train)), replace = FALSE)
#split rows into train and cross validation set.
set.seed(1)
train <- sample(1:nrow(df.train), round(.8*nrow(df.train)), replace = FALSE)
cv <-  1:nrow(df.train)[-train]
train
1:900
1:900[train]
df.train[train, ]
formula <- Survived ~
Pclass +
Sex +
Age +
SibSp +
Parch
formula
glm
model.log.regession <- glm(formula = formula, data = df.train[train,], family = binomial)
model.log.regession <- glm(formula = formula, data = df.train[train,], family = binomial)
summary(model.log.regession)
df.train[-train, ]
cv.probs <- predict(model.log.regession, newdata = df.train[-train, ], type = "response")
cv.probs <- predict(model.log.regession, newdata = df.train[-train, ], type = "response")
head(cv.probs)
table(cv.predictions, Y)
# Convert logistic regression output to predictions
cv.predictions <- rep(0, length(cv.probs))
cv.predictions[cv.probs > .5] = 1
# extract true cross validated response
Y <- df.train[-train, "Survived"]
table(cv.predictions, Y)
cv.predictions != Y
sum(cv.predictions != Y)
mean(cv.predictions != Y)
cv.predictions == 1 & Y == 1
sum(cv.predictions == 1 & Y == 1)/sum(cv.predictions == 1)
sum(cv.predictions == 1 % Y == 1)/sum(Y == 1)
sum(cv.predictions == 1 & Y == 1)/sum(Y == 1)
classification.error <- function(pred, Y){
return(mean(pred != Y))
}
precision <- function(pred, Y){
return( sum(pred == 1 & Y ==1) / sum(pred == 1))
}
recall <-  function(pred, Y){
return(sum(pred == 1 & Y == 1)/sum(Y == 1))
}
F1.score(pred, Y){
classification.error <- function(pred, Y){
return(mean(pred != Y))
}
precision <- function(pred, Y){
return( sum(pred == 1 & Y ==1) / sum(pred == 1))
}
recall <-  function(pred, Y){
return(sum(pred == 1 & Y == 1)/sum(Y == 1))
}
F1.score <- function(pred, Y){
prec <- precision(pred, Y)
rec <- recall(pred, Y)
return(2*(prec*rec)/(prec + rec))
}
classification.error(cv.predictions, Y)
F1.score(cv.predictions, Y)
paste("Classification error: ", classification.error(cv.predictions, Y), "\n",
"F1 Score: ", F1.score(cv.predictions, Y))
paste("Classification error: ", classification.error(cv.predictions, Y), sprintf("\n"),
"F1 Score: ", F1.score(cv.predictions, Y))
print("Classification error: ", classification.error(cv.predictions, Y))
print("F1 Score: ", F1.score(cv.predictions, Y))
print("Classification error: %l", classification.error(cv.predictions, Y))
print("F1 Score: %l", F1.score(cv.predictions, Y))
sprintf("Classification error: %l", classification.error(cv.predictions, Y))
sprintf("Classification error: %d", classification.error(cv.predictions, Y))
sprintf("Classification error: %f", classification.error(cv.predictions, Y))
sprintf("F1 Score: %f", F1.score(cv.predictions, Y))
table(cv.predictions, Y)
sprintf("Classification error: %f", classification.error(cv.predictions, Y))
sprintf("F1 Score: %f", F1.score(cv.predictions, Y))
formula <- Survived ~
Pclass +
Sex +
Age +
SibSp
model.log.regession <- glm(formula = formula, data = df.train[train,], family = binomial)
summary(model.log.regession)
cv.probs <- predict(model.log.regession, newdata = df.train[-train, ], type = "response")
head(cv.probs)
# Convert logistic regression output to predictions
cv.predictions <- rep(0, length(cv.probs))
cv.predictions[cv.probs > .5] = 1
# extract true cross validated response
Y <- df.train[-train, "Survived"]
# Confusion Matrix (to correct classifcations, type 1 and type 2 errors)
table(cv.predictions, Y)
classification.error <- function(pred, Y){
return(mean(pred != Y))
}
precision <- function(pred, Y){
return( sum(pred == 1 & Y ==1) / sum(pred == 1))
}
recall <-  function(pred, Y){
return(sum(pred == 1 & Y == 1)/sum(Y == 1))
}
F1.score <- function(pred, Y){
prec <- precision(pred, Y)
rec <- recall(pred, Y)
return(2*(prec*rec)/(prec + rec))
}
table(cv.predictions, Y)
sprintf("Classification error: %f", classification.error(cv.predictions, Y))
sprintf("F1 Score: %f", F1.score(cv.predictions, Y))
table(cv.predictions, Y)
sprintf("Classification error: %f", classification.error(cv.predictions, Y))
sprintf("F1 Score: %f", F1.score(cv.predictions, Y))
length(Y)
length(cv.predictions)
model.age <- tree(data = df.train, formula = Age ~ Pclass + Parch + SibSp)
library(tree)
model.age <- tree(data = df.train, formula = Age ~ Pclass + Parch + SibSp)
plot(model.age)
text(model.age
)
cv.predictions
cv.predictions == Y
df.train[-train,]
df.errors <-  (df.train[-train, ])[cv.predictions != Y, ]
df.errors
View(df.errors)
sum(is.na(df.errors$Age))
sum(is.na(df.train[-train,]$Age))
mean(is.na(df.errors$Age))
mean(is.na(df.train[-train,]$Age))
df.cv <- df.train[-train,]
df.errors <-  df.cv[cv.predictions != Y, ]
df.train
df.train
df.cv
df.cv %>% group_by(Embarked) %>% summarize(perc.station <- n()/nrow(df.cv))
df.errors %>% group_by(Embarked) %>% summarize(perc.station <- n()/nrow(df.errors))
ggplot(data = df.train, aes(x= Fare)) + geom_hist() + facet_wrap(~Pclass)
ggplot(data = df.train, aes(x= Fare)) + geom_histogram() + facet_wrap(~Pclass)
ggplot(data = df.train, aes(x= Fare)) + geom_histogram(fill = Survived) + facet_wrap(~Pclass)
ggplot(data = df.train, aes(x= Fare, y = Sex)) + geom_point(fill = Survived) + facet_wrap(~Pclass)
ggplot(data = df.train, aes(x= Fare, y = Sex)) + geom_point() + facet_wrap(~Pclass)
ggplot(data = df.train, aes(x= Fare, y = Sex, fill = Survived)) + geom_point(fill = Survived) + facet_wrap(~Pclass)
ggplot(data = df.train, aes(x= Fare, y = Sex, color = Survived)) + geom_point(fill = Survived) + facet_wrap(~Pclass)
ggplot(data = df.train, aes(x= Fare)) + geom_histogram() + facet_wrap(~Pclass)
df.train %>% filter(Pclass = 3) %>% groupby(Survived) %>% summarize(avg.Fare = mean(Fare))
df.train %>% filter(Pclass == 3) %>% groupby(Survived) %>% summarize(avg.Fare = mean(Fare))
df.train %>% filter(Pclass == 3) %>% group_by(Survived) %>% summarize(avg.Fare = mean(Fare))
df.train  %>% group_by(Pclass, Survived) %>% summarize(avg.Fare = mean(Fare))
plot(model.age)
plot(model.age)
text(model.age)
library(randomForrest)
install.package('randomForrest')
install.packages('RandomForrest')
require(RandomForrest)
df.errors
View(df.errors %>% arrange(desc(Survived), desc(Pclass), desc(Age))
)
View(df.errors %>% arrange(asc(Survived), asc(Pclass), asc(Age)))
View(df.errors %>% arrange((Survived), (Pclass), (Age)))
View(df.errors %>% arrange((Survived), (Pclass), (Sex), (Age)))
?seq
nrow(df.train)
nrow(df.train)/10
round(nrow(df.train)/10)
round(nrow(df.train)/20)
?round
floor(nrow(df.train)/20)
l <- list()
l[1] = list(a <- 10, b <- 20)
l[1]
l
l
?list
list()
list()
a <- list()
a
a[1]
a[2]
a
a[1] <- list(x = 1, y = 2)
a[[1]] <- list(x = 1, y = 2)
a[[2]] <- list(x = 1, y = 2)
a
as.data.frame(a)
a[1] <- list(1, 2)
a
a[[1]] <- list(1, 2)
a[[2]] <- list(1, 2)
a
as.data.frame(a)
plyr
library(plyr)
library(dplyr)
rm(plyr)
rm('plyr')
df.train %>% arrange(Age)
a
data.frame(matrix(unlist(a)), nrow = NROW(a), byrow = T)
data.frame(matrix(unlist(a)), nrow = NROW(a))
a
data.frame(matrix(unlist(a)), nrow = NROW(a))
data.frame(matrix(unlist(a), nrow = NROW(a)))
data.frame(matrix(unlist(a), nrow = NROW(a)), byrow = T)
data.frame(matrix(unlist(a, byrow = T), nrow = NROW(a),))
data.frame(matrix(unlist(a), nrow = NROW(a)), byrow = T)
data.frame(matrix(unlist(a), nrow = NROW(a), byrow = T))
a[[1]] = list(x = 1, y = 2)
a[[2]] = list(x = 1, y = 2)
data.frame(matrix(unlist(a), nrow = NROW(a), byrow = T))
formula
vecotr
cv.probs
ifelse(cv.probs > .5, 1, 0)
class(ifelse(cv.probs > .5, 1, 0))
cv.predictions
a = b = 1
a
b
?" <- "
?<-
<-
??"<-"
View(classification.error)
logistic.learning.curve <-  function(df.train, formula, seed = 1, classification.error = TRUE){
# Define training and cross validation sets
set.seed(seed)
train <- sample(1:nrow(df.train), round(.8*nrow(df.train)), replace = FALSE)
df.cv <- df.train[-train, ]
# take subsets in the data to train classifier with, each with an additional
# chunk of the data one twentieth the size of the training set. Save errors.
errors <- list()
chunk.size <- floor(nrow(df.train)/20)
for( i in 1:20){
# Take a subset of training set
training.subset <- (df.train[train,])[1:(i*chunk.size),]
# Fit a logistic regression classifier with inputed formula
logistic.model <- glm(formula = formula, data = training.subset, family = binomial)
# Predict responses for training subset and cross validation set
training.subset.probs <- predict(logistic.model, newdata <- training.subset, type = "response")
cv.probs <- predict(logistic.model, newdata = df.cv, type = "response")
training.subset.predictions  <-  rep(0, length(training.subset.probs))
cv.predictions <- rep(0, length(cv.probs))
training.subset.predictions[training.subset.probs > .5] = 1
cv.predictions[cv.probs > .5] = 1
# store true responses
Y.train <- (df.train[train, ])[1:(i*chunk.size), "Survived"]
Y.cv <- df.cv$Survived
# Store error. If parameter 'classification.error' is TRUE (default), use classificaton error.
# Otherwise use F1 score.
if(classification.error){
errors[[i]] <- list(chunk = i,
training.error = classification.error(
pred = training.subset.predictions, Y = Y.train),
cv.error = classification.error(
pred = cv.predictions, Y = Y.cv)
)
}
else{
errors[[i]] <- list(chunk = i,
training.error = F1.score(
pred = training.subset.predictions, Y = Y.train),
cv.error = F1.score(
pred = cv.predictions, Y = Y.cv)
)
}
}
return(errors)
}
logistic.learning.curve(df.train = df.train, formula = formula)
a = logistic.learning.curve(df.train = df.train, formula = formula)
data.frame(matrix(unlist(a), byrow = T, nrow = NROW(a)))
training.subset.probs
?data.frame
b <- data.frame(matrix(unlist(a), byrow = T, nrow = NROW(a)))
b
names(b)
names(b) <- c("Chunk", "training.error", "cv.error")
b
ggplot(data = b)
ggplot(data = b) + geom_line(aes(x = Chunk, y = training.error))
ggplot(data = b) + geom_line(aes(x = Chunk, y = training.error), color = "red")
ggplot(data = b) + geom_line(aes(x = Chunk, y = training.error), color = "red") + geom_line(aes(x = Chunk, y = cv.error), color = "blue")
logistic.learning.curve <-  function(df.train, formula, seed = 1, classification.error = TRUE){
# Define training and cross validation sets
set.seed(seed)
train <- sample(1:nrow(df.train), round(.8*nrow(df.train)), replace = FALSE)
df.cv <- df.train[-train, ]
# take subsets in the data to train classifier with, each with an additional
# chunk of the data one fiftieth the size of the training set. Save errors.
errors <- list()
chunk.size <- floor(nrow(df.train)/50)
for( i in 1:50){
# Take a subset of training set
training.subset <- (df.train[train,])[1:(i*chunk.size),]
# Fit a logistic regression classifier with inputed formula
logistic.model <- glm(formula = formula, data = training.subset, family = binomial)
# Predict responses for training subset and cross validation set
training.subset.probs <- predict(logistic.model, newdata <- training.subset, type = "response")
cv.probs <- predict(logistic.model, newdata = df.cv, type = "response")
training.subset.predictions  <-  rep(0, length(training.subset.probs))
cv.predictions <- rep(0, length(cv.probs))
training.subset.predictions[training.subset.probs > .5] = 1
cv.predictions[cv.probs > .5] = 1
# store true responses
Y.train <- (df.train[train, ])[1:(i*chunk.size), "Survived"]
Y.cv <- df.cv$Survived
# Store error. If parameter 'classification.error' is TRUE (default), use classificaton error.
# Otherwise use F1 score.
if(classification.error){
errors[[i]] <- list(chunk = i,
training.error = classification.error(
pred = training.subset.predictions, Y = Y.train),
cv.error = classification.error(
pred = cv.predictions, Y = Y.cv)
)
}
else{
errors[[i]] <- list(chunk = i,
training.error = F1.score(
pred = training.subset.predictions, Y = Y.train),
cv.error = F1.score(
pred = cv.predictions, Y = Y.cv)
)
}
}
return(errors)
}
a = logistic.learning.curve(df.train = df.train, formula = formula)
b <- data.frame(matrix(unlist(a), byrow = T, nrow = NROW(a)))
names(b) <- c("Chunk", "training.error", "cv.error")
ggplot(data = b) + geom_line(aes(x = Chunk, y = training.error), color = "red") + geom_line(aes(x = Chunk, y = cv.error), color = "blue")
a = logistic.learning.curve(df.train = df.train, formula = formula, classification.error = F)
b <- data.frame(matrix(unlist(a), byrow = T, nrow = NROW(a)))
names(b) <- c("Chunk", "training.error", "cv.error")
ggplot(data = b) + geom_line(aes(x = Chunk, y = training.error), color = "red") + geom_line(aes(x = Chunk, y = cv.error), color = "blue")
lc.list.classification_error <- logistic.learning.curve(df.train, formula, seed = 1, classification.error = TRUE)
lc.list.F1_score <- logistic.learning.curve(df.train, formula, seed = 1, classification.error = FALSE)
lc.df.classification_error <- data.frame(matrix(unlist(lc.list.classification_error), byrow = TRUE, nrow = NROW(lc.list.classification_error)))
lc.df.F1_score <- data.frame(matrix(unlist(lc.list.F1_score), byrow = TRUE, nrow = NROW(lc.list.F1_score)))
lc.list.classification_error
lc.list.classification_error <- logistic.learning.curve(df.train, formula, seed = 1, classification.error = TRUE)
lc.list.F1_score <- logistic.learning.curve(df.train, formula, seed = 1, classification.error = FALSE)
lc.df.classification_error <- data.frame(matrix(unlist(lc.list.classification_error), byrow = TRUE, nrow = NROW(lc.list.classification_error)))
lc.df.F1_score <- data.frame(matrix(unlist(lc.list.F1_score), byrow = TRUE, nrow = NROW(lc.list.F1_score)))
names(lc.df.classification_error) = names(lc.df.F1_score) = c("Chunk", "training.error", "cv.error")
lc.df.F1_score
g_classification.lc <- ggplot(data = lc.df.classification_error, aes(x = Chunk, y = training.error))
+ geom_line(aes(x = Chunk, y = training.error)) + geom_line(aes(x = Chunk, y = cv.error))
g_classification.lc <- ggplot(data = lc.df.classification_error, aes(x = Chunk, y = training.error))
+ geom_line()
?geom_line
g_classification.lc <- ggplot(data = lc.df.classification_error, aes(x = Chunk, y = training.error)) + geom_line()
g_classification.lc
g_classification.lc <- ggplot(data = lc.df.classification_error, aes(x = Chunk, y = training.error)) + geom_line() + geom_line(aes(x= Chunk, y = cv.error))
g_classification.lc
g_classification.lc <- ggplot(data = lc.df.classification_error) + geom_line( aes(x = Chunk, y = training.error)) + geom_line(aes(x= Chunk, y = cv.error))
g_classification.lc
